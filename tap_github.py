import datetime
import dateutil.parser
import os
import json
import pytz
import requests
import urllib

import singer
import singer.metrics as metrics

# need to make the timestamp timezone aware for comparison below
_MIN_TS = pytz.utc.localize(datetime.datetime.min)

ACCESS_TOKEN = 'access_token'
REPOSITORY = 'repository'

COMMITS = 'commits'
ISSUES = 'issues'
ASSIGNEES = 'assignees'
COLLABORATORS = 'collaborators'
PULL_REQUESTS = 'pull_requests'
STARGAZERS = 'stargazers'

REQUIRED_CONFIG_KEYS = [ACCESS_TOKEN, REPOSITORY]

KEY_PROPERTIES = {
    COMMITS: ['sha'],
    ISSUES: ['id'],
    ASSIGNEES: ['id'],
    COLLABORATORS: ['id'],
    PULL_REQUESTS: ['id'],
    STARGAZERS: ['user_id']
}

session = requests.Session()
logger = singer.get_logger()


def clean_tz(dt):
    '''
    Make Python datetime objects Github endpoint friendly by removing the
    '+00:00' generated by the isoformat() method and replace it with a 'Z'.
    This assumes that your timezones are UTC.
    '''
    return dt.isoformat().replace('+00:00', 'Z')


def authed_get(source, url, headers={}):
    with metrics.http_request_timer(source) as timer:
        session.headers.update(headers)
        resp = session.request(method='get', url=url)
        timer.tags[metrics.Tag.http_status_code] = resp.status_code
        return resp

def authed_get_all_pages(source, url, headers={}):
    while True:
        r = authed_get(source, url, headers)
        yield r
        if 'next' in r.links:
            url = r.links['next']['url']
        else:
            break

def get_abs_path(path):
    return os.path.join(os.path.dirname(os.path.realpath(__file__)), path)

def load_schemas():
    schemas = {}

    for filename in os.listdir(get_abs_path('tap_github')):
        path = get_abs_path('tap_github') + '/' + filename
        file_raw = filename.replace('.json', '')
        with open(path) as file:
            schemas[file_raw] = json.load(file)

    return schemas


def write_metadata(metadata, values, breadcrumb):
    metadata.append(
        {
            'metadata': values,
            'breadcrumb': breadcrumb
        }
    )

def populate_metadata(schema, metadata, breadcrumb, key_properties):

    # if object, recursively populate object's 'properties'
    if 'object' in schema['type']:
        for prop_name, prop_schema in schema['properties'].items():
            prop_breadcrumb = breadcrumb + ['properties', prop_name]
            populate_metadata(prop_schema, metadata, prop_breadcrumb, key_properties)

    # otherwise, mark as available unless a key property, then automatic
    else:
        prop_name = breadcrumb[-1]
        inclusion = 'automatic'
        # for field selection
        #inclusion = 'automatic' if prop_name in key_properties else 'available'
        values = {'inclusion': inclusion}
        write_metadata(metadata, values, breadcrumb)

def get_catalog():
    raw_schemas = load_schemas()
    streams = []

    for schema_name, schema in raw_schemas.items():

        # get metadata for each field
        metadata = []
        populate_metadata(schema, metadata, [], KEY_PROPERTIES[schema_name])

        # create and add catalog entry
        catalog_entry = {
            'stream': schema_name,
            'tap_stream_id': schema_name,
            'schema': schema,
            'metadata' : metadata,
            'key_properties': KEY_PROPERTIES[schema_name],
        }
        streams.append(catalog_entry)

    return {'streams': streams}

def do_discover():
    catalog = get_catalog()
    # dump catalog
    print(json.dumps(catalog, indent=2))

def get_all_pull_requests(stream, config, state):
    '''
    https://developer.github.com/v3/pulls/#list-pull-requests
    '''
    query = urllib.parse.urlencode({'state': 'all',
                                    'sort': 'updated',
                                    'direction': 'asc'})
    repo = config[REPOSITORY]
    pr_state = state.get(PULL_REQUESTS)
    pr_state = dateutil.parser.parse(pr_state) if pr_state else _MIN_TS
    with metrics.record_counter(PULL_REQUESTS) as counter:
        url = 'https://api.github.com/repos/{}/pulls?{}'.format(repo, query)
        for response in authed_get_all_pages(PULL_REQUESTS, url):
            pull_requests = response.json()
            extraction_time = singer.utils.now()
            for pr in pull_requests:
                if dateutil.parser.parse(pr['updated_at']) > pr_state:
                    rec = singer.transform(pr, stream)
                    singer.write_record(PULL_REQUESTS,
                                        rec,
                                        time_extracted=extraction_time)
                    counter.increment()
    # handle the case when there are no PRs to pull which means no pr object
    try:
        state[PULL_REQUESTS] = pr['updated_at']
    except NameError:
        state[PULL_REQUESTS] = state.get(PULL_REQUESTS)
    return state

def get_all_assignees(stream, config, state):
    '''
    https://developer.github.com/v3/issues/assignees/#list-assignees
    '''
    repo_path = config[REPOSITORY]
    with metrics.record_counter(ASSIGNEES) as counter:
        for response in authed_get_all_pages(ASSIGNEES, 'https://api.github.com/repos/{}/assignees'.format(repo_path)):
            assignees = response.json()
            extraction_time = singer.utils.now()
            for assignee in assignees:
                rec = singer.transform(assignee, stream)
                singer.write_record(ASSIGNEES, rec, time_extracted=extraction_time)
                counter.increment()

    return state

def get_all_collaborators(stream, config, state):
    '''
    https://developer.github.com/v3/repos/collaborators/#list-collaborators
    '''
    repo_path = config[REPOSITORY]
    with metrics.record_counter(COLLABORATORS) as counter:
        for response in authed_get_all_pages(COLLABORATORS, 'https://api.github.com/repos/{}/collaborators'.format(repo_path)):
            collaborators = response.json()
            extraction_time = singer.utils.now()
            for collaborator in collaborators:
                rec = singer.transform(collaborator, stream)
                singer.write_record(COLLABORATORS, rec, time_extracted=extraction_time)
                counter.increment()

    return state

def get_all_commits(stream, config, state):
    '''
    https://developer.github.com/v3/repos/commits/#list-commits-on-a-repository
    '''
    repo_path = config[REPOSITORY]
    query_string = ''
    if COMMITS in state and state[COMMITS] is not None:
        query_string = '?since={}'.format(state[COMMITS])

    ts_state = _MIN_TS
    with metrics.record_counter(COMMITS) as counter:
        for response in authed_get_all_pages(COMMITS, 'https://api.github.com/repos/{}/commits{}'.format(repo_path, query_string)):
            commits = response.json()
            extraction_time = singer.utils.now()
            for commit in commits:
                commit_date = dateutil.parser.parse(commit['commit']['author']['date'])
                ts_state = max(commit_date, ts_state)
                rec = singer.transform(commit, stream)
                singer.write_record(COMMITS, rec, time_extracted=extraction_time)
                counter.increment()
    try:
        ts_state = clean_tz(ts_state)
    except UnboundLocalError:
        ts_state = state.get(COMMITS)
    state[COMMITS] = ts_state
    return state

def get_all_issues(stream, config,  state):
    '''
    https://developer.github.com/v3/issues/#list-issues-for-a-repository
    '''
    repo_path = config[REPOSITORY]
    params = {'sort': 'updated',
              'direction': 'asc'}
    if ISSUES in state and state[ISSUES] is not None:
        params['since'] = format(state[ISSUES])
    query = urllib.parse.urlencode(params)
    url = 'https://api.github.com/repos/{}/issues?{}'.format(repo_path, query)
    with metrics.record_counter(ISSUES) as counter:
        for response in authed_get_all_pages(ISSUES, url):
            issues = response.json()
            extraction_time = singer.utils.now()
            for issue in issues:
                updated_at = dateutil.parser.parse(issue['updated_at'])
                ts_state = max(updated_at, _MIN_TS)
                rec = singer.transform(issue, stream)
                singer.write_record(ISSUES, rec, time_extracted=extraction_time)
                counter.increment()
    try:
        state[ISSUES] = ts_state.isoformat()
    except UnboundLocalError:
        state[ISSUES] = state.get(ISSUES)
    return state

def get_all_stargazers(stream, config, state):
    '''
    https://developer.github.com/v3/activity/starring/#list-stargazers
    '''
    repo_path = config[REPOSITORY]
    if STARGAZERS in state and state[STARGAZERS] is not None:
        query_string = '&since={}'.format(state[STARGAZERS])
    else:
        query_string = ''

    stargazers_headers = {'Accept': 'application/vnd.github.v3.star+json'}
    ts_state = _MIN_TS
    with metrics.record_counter(STARGAZERS) as counter:
        url = 'https://api.github.com/repos/{}/stargazers?sort=updated&direction=asc{}'.format(repo_path, query_string)
        for response in authed_get_all_pages(STARGAZERS, url, stargazers_headers):
            stargazers = response.json()
            extraction_time = singer.utils.now()
            for stargazer in stargazers:
                starred_at = dateutil.parser.parse(stargazer['starred_at'])
                ts_state = max(starred_at, ts_state)
                rec = singer.transform(stargazer, stream)
                rec['user_id'] = rec['user']['id']
                singer.write_record(STARGAZERS, rec, time_extracted=extraction_time)
                counter.increment()
    try:
        state[STARGAZERS] = ts_state.isoformat()
    except UnboundLocalError:
        state[STARGAZERS] = state.get(STARGAZERS)
    return state

def get_selected_streams(catalog):
    '''
    Gets selected streams.  Checks schema's 'selected'
    first -- and then checks metadata, looking for an empty
    breadcrumb and mdata with a 'selected' entry
    '''
    selected_streams = []
    for stream in catalog['streams']:
        stream_metadata = stream['metadata']
        if stream['schema'].get('selected', False):
            selected_streams.append(stream['tap_stream_id'])
        else:
            for entry in stream_metadata:
                # stream metadata will have empty breadcrumb
                if not entry['breadcrumb'] and entry['metadata'].get('selected',None):
                    selected_streams.append(stream['tap_stream_id'])

    return selected_streams



SYNC_FUNCTIONS = {
    COMMITS: get_all_commits,
    ISSUES: get_all_issues,
    ASSIGNEES: get_all_assignees,
    COLLABORATORS: get_all_collaborators,
    PULL_REQUESTS: get_all_pull_requests,
    STARGAZERS: get_all_stargazers
}

def do_sync(config, state, catalog):
    access_token = config[ACCESS_TOKEN]
    session.headers.update({'authorization': 'token ' + access_token})

    # get selected streams
    selected_stream_ids = get_selected_streams(catalog)

    # loop over streams
    for catalog_entry in catalog['streams']:
        stream_id = catalog_entry['tap_stream_id']
        stream_schema = catalog_entry['schema']

        # if stream is selected, write schema and sync
        if stream_id in selected_stream_ids:
            singer.write_schema(
                stream_id,
                catalog_entry['schema'],
                catalog_entry['key_properties']
            )
            sync_func = SYNC_FUNCTIONS[stream_id]
            sync_func(stream_schema, config, state)

    singer.write_state(state)

@singer.utils.handle_top_exception(logger)
def main():
    args = singer.utils.parse_args(REQUIRED_CONFIG_KEYS)
    if args.discover:
        do_discover()
    else:
        catalog = args.properties if args.properties else get_catalog()
        do_sync(args.config, args.state, catalog)

if __name__ == '__main__':
    main()
